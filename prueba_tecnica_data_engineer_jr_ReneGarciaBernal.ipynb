{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba Téncnica - ArkonData\n",
    "## Data Engineer Jr\n",
    "### René García Bernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cree una base de datos local con los conjuntos de datos proporcionados,  \n",
    "puede utilizar la herramienta de gestión y administración de base de datos de su preferencia,  \n",
    "queda a su consideración la gestión de esta base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create logger function decorator to monitor all the other proccesses\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "# Configure the logging system\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the logging decorator\n",
    "def logger(func):\n",
    "    @wraps(func)  # Preserve the original function's metadata (name, docstring)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info(f\"Started executing '{func.__name__}' with arguments {args} and {kwargs}\")\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            logging.info(f\"Successfully finished '{func.__name__}' with result: {result}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in '{func.__name__}': {e}\", exc_info=True)\n",
    "            raise  # Re-raise the exception after logging\n",
    "        finally:\n",
    "            logging.info(f\"Finished execution of '{func.__name__}'\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text, update, and_\n",
    "from sqlalchemy.schema import CreateTable, CreateSchema, DropSchema, DropTable, Table, Column, MetaData\n",
    "from sqlalchemy.types import *\n",
    "import pandas as pd\n",
    "\n",
    "class Connector:\n",
    "\n",
    "    def __init__(self,credentials):\n",
    "        self._host = credentials[\"host\"]\n",
    "        self._port = credentials[\"port\"]\n",
    "        self._user = credentials[\"user\"]\n",
    "        self._password = credentials[\"password\"]\n",
    "        self._database = credentials[\"database\"]\n",
    "        self.schema = credentials[\"schema\"]\n",
    "        self._dialect = credentials[\"dialect\"]\n",
    "        self._driver = credentials[\"driver\"]\n",
    "        self._engine = self._create_engine()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"host: {self._host}. port: {self._port}. user: {self._user}. database: {self._database}.\"\n",
    "\n",
    "    def _create_engine(self):\n",
    "        url = (\n",
    "            f\"{self._dialect}+{self._driver}://{self._user}:\"\n",
    "            f\"{self._password}@{self._host}:{self._port}/\"\n",
    "            f\"{self._database}\"\n",
    "        )\n",
    "        return create_engine(url)\n",
    "    \n",
    "    @logger\n",
    "    def test_conection(self):\n",
    "        try:\n",
    "            conn = self._engine.connect()\n",
    "            conn.close()\n",
    "            return f\"Connectio to database {self._database} successful\"\n",
    "        except Exception as e:\n",
    "            return f\"Connection failed: {e}\"\n",
    "    \n",
    "    @logger\n",
    "    def create_schema(self,schema):\n",
    "        try:\n",
    "            with self._engine.connect() as conn:\n",
    "                stmt = CreateSchema(schema,if_not_exists=True)\n",
    "                with conn.begin():\n",
    "                    conn.execute(stmt)\n",
    "            return stmt\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {e}\"\n",
    "        return None\n",
    "    \n",
    "    @logger\n",
    "    def create_table(self,table_name,table_deff):\n",
    "        \"\"\"\n",
    "        Table will be created in to the schema self._schema\n",
    "        table_deff should be of dict type with the form\n",
    "        key = column name\n",
    "        value = data type and nullable constraint\n",
    "        example: columns = {\n",
    "            \"Column1\":{\"type\": INTEGER, \"nullable\": false},\n",
    "            \"Column2\":{\"type\": VARCHAR, \"nullable\": false},\n",
    "            \"Column3\":{\"type\": VARCHAR, \"nullable\": false},\n",
    "            \"Column4\":{\"type\": VARCHAR, \"nullable\": true}\n",
    "        } \n",
    "        dataframe is a pandas dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self._engine.connect() as conn:\n",
    "                if isinstance(table_deff,pd.DataFrame):\n",
    "                    table_deff.to_sql(table_name,conn,schema=self.schema,if_exists=\"replace\",index=False)\n",
    "                    \n",
    "                elif isinstance(table_deff,dict):\n",
    "                    columns_list = self._create_columns(table_deff)\n",
    "                    metadata = MetaData(self.schema)\n",
    "                    table = Table(table_name,metadata,*columns_list,schema=self.schema)\n",
    "                    stmt = CreateTable(table,if_not_exists=True)                    \n",
    "                    with conn.begin():\n",
    "                        conn.execute(stmt)\n",
    "            return \"Finished without errors\"\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {e}\"\n",
    "\n",
    "    @logger\n",
    "    def drop_schema(self,schema):\n",
    "        try:\n",
    "            with self._engine.connect() as conn:\n",
    "                stmt = DropSchema(schema,if_exists=True)\n",
    "                with conn.begin():\n",
    "                    conn.execute(stmt)\n",
    "                return stmt\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {e}\"\n",
    "        return None\n",
    "    \n",
    "    @logger\n",
    "    def drop_table(self,table_name):\n",
    "        try:\n",
    "            with self._engine.connect() as conn:\n",
    "                table1meta = MetaData()\n",
    "                table = Table(table_name, table1meta, autoload_with=self._engine)\n",
    "                stmt = DropTable(table,if_exists=True)\n",
    "                with conn.begin():\n",
    "                    conn.execute(stmt)\n",
    "                return stmt\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {e}\"\n",
    "\n",
    "    @logger    \n",
    "    def execute_sql(self,sqlquery):\n",
    "        try:\n",
    "            result = []\n",
    "            with self._engine.connect() as conn:\n",
    "                stmt = text(sqlquery)\n",
    "                conn.begin()\n",
    "                cursor = conn.execute(stmt)\n",
    "                conn.commit()\n",
    "                result.append(tuple(cursor.keys()))\n",
    "                for row in cursor:\n",
    "                    result.append(row)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    @logger\n",
    "    def update(self,schema,table_nm,where,values):\n",
    "        try:\n",
    "            with self._engine.connect() as conn:\n",
    "                table1meta = MetaData(schema)\n",
    "                table = Table(table_nm, table1meta, autoload_with=conn)\n",
    "                where_clause = and_(*[table.c[key] == value for key, value in where.items()])\n",
    "                stmt = update(table).where(where_clause).values(values)\n",
    "                conn.execute(stmt)\n",
    "                conn.commit()\n",
    "                return stmt\n",
    "        except Exception as e:\n",
    "            return f\"Error in update statmenet: {e}\"\n",
    "\n",
    "    def get_connection(self):\n",
    "        return self._engine.connect()\n",
    "    \n",
    "    def _create_columns(self,column_deff):\n",
    "        columns = [\n",
    "            Column(key,value['type'],nullable=value['nullable'])\n",
    "            for key, value in column_deff.items()\n",
    "        ]\n",
    "\n",
    "        return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser as cnf\n",
    "\n",
    "config = cnf.ConfigParser()\n",
    "config.read('database_credentials.ini')\n",
    "\n",
    "connection = Connector(config[\"postgresql\"])\n",
    "connection.test_conection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates schema to store raw data\n",
    "stg_schema = \"staging\"\n",
    "connection.create_schema(stg_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Creates pandas dataframe from csv file \n",
    "data1 = pd.read_csv(filepath_or_buffer='datasets/Data1.csv',header=0)\n",
    "\n",
    "#Creates a pandas dataframe from parquet file\n",
    "data2 = pd.read_parquet(path='datasets/data2.parquet',engine='pyarrow')\n",
    "\n",
    "#Explore data, column names, data types, row count.\n",
    "print(f\"Headers from csv file: {data1.columns}\",f\"row count: {data1.shape[0]}\")\n",
    "\n",
    "print(f\"Headers from parquet file: {data2.columns}\",f\"row count: {data2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tables from files into the database \n",
    "table_csv = \"stg_starwars_csv\"\n",
    "table_parquet = \"stg_starwars_prqt\"\n",
    "table_union = \"starwars_union\"\n",
    "connection.schema = stg_schema\n",
    "connection.create_table(table_csv,data1)\n",
    "connection.create_table(table_parquet,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete this block of code\n",
    "data1.to_sql(\"stg_starwars_csv\",connection.get_connection(),if_exists=\"replace\",index=False)\n",
    "data2.to_sql(\"stg_starwars_parquet\",connection.get_connection(),if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cree una tercera tabla en dónde unifique ambos conjuntos de datos,  \n",
    "asegúrese de no tener duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new schema with clean data\n",
    "rfnd_schema = \"refined\"\n",
    "connection.create_schema(rfnd_schema)\n",
    "connection.schema = rfnd_schema\n",
    "\n",
    "data_union = pd.concat([data1,data2],ignore_index=True,sort=False)\n",
    "data_union.drop_duplicates(keep='first',inplace=True)\n",
    "data_union.duplicated()\n",
    "\n",
    "connection.create_table(table_union,data_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generar una consulta con los nombres duplicados y la cantidad de veces que se repiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with pandas\n",
    "sql_query = (\n",
    "    \"SELECT name, count(*)\"    \n",
    "    f\"FROM ( SELECT name FROM {stg_schema}.{table_csv} UNION ALL \" \n",
    "    f\"SELECT name FROM {stg_schema}.{table_parquet}) t \"\n",
    "    \"GROUP BY name HAVING COUNT(name) > 1;\"\n",
    ")\n",
    "\n",
    "repeated_names = pd.read_sql_query(sql_query,connection.get_connection())\n",
    "\n",
    "print(repeated_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with Connector object\n",
    "sql_query = (\n",
    "    \"SELECT name, count(*)\"    \n",
    "    f\"FROM ( SELECT name FROM {stg_schema}.{table_csv} UNION ALL \" \n",
    "    f\"SELECT name FROM {stg_schema}.{table_parquet}) t \"\n",
    "    \"GROUP BY name HAVING COUNT(name) > 1;\"\n",
    ")\n",
    "\n",
    "repeated_names = connection.execute_sql(sql_query)\n",
    "print(repeated_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Genere una consulta SQL que devuelva los nombres de las personas cuyo height esté entre 180 y 190,  \n",
    "cuyo gender sea male y cuyo hair_color sea diferente de none o cualquier valor null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with pandas\n",
    "sql_query = (\n",
    "    \"SELECT name \"    \n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"WHERE height >= 180 and height <= 190 \" \n",
    "    \"AND gender = 'masculine' \"\n",
    "    \"AND (hair_color is not null or hair_color not in ('','null','None','none'));\"\n",
    ")\n",
    "\n",
    "names_filtered_by_hieght = pd.read_sql_query(sql_query,connection.get_connection())\n",
    "\n",
    "print(names_filtered_by_hieght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with connector object\n",
    "sql_query = (\n",
    "    \"SELECT name \"    \n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"WHERE height >= 180 and height <= 190 \" \n",
    "    \"AND gender = 'masculine' \"\n",
    "    \"AND (hair_color is not null or hair_color not in ('','null','None','none'));\"\n",
    ")\n",
    "\n",
    "names_filtered_by_hieght = connection.execute_sql(sql_query)\n",
    "print(repeated_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Escribir una consulta SQL que genera una columna de bandera (flag),  \n",
    "donde se asigna el valor booleano o integer 1 si el mass es superior al promedio,  \n",
    "y 0 si el mass es menor o igual al promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution wiht Connector object\n",
    "sql_query = (\n",
    "    f\"SELECT *, CASE WHEN mass > (SELECT AVG(mass) FROM {rfnd_schema}.{table_union}) \"\n",
    "    \"THEN 1 ELSE 0 END AS flag \"    \n",
    "    f\"FROM {rfnd_schema}.{table_union};\"\n",
    ")\n",
    "\n",
    "flag = pd.read_sql_query(sql_query,connection.get_connection())\n",
    "\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = (\n",
    "    f\"SELECT *, CASE WHEN mass > (SELECT AVG(mass) FROM {rfnd_schema}.{table_union}) \"\n",
    "    \"THEN 1 ELSE 0 END AS flag \"    \n",
    "    f\"FROM {rfnd_schema}.{table_union};\"\n",
    ")\n",
    "\n",
    "flag = connection.execute_sql(sql_query)\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cree una tabla en dónde inserte los valores únicos de la columna Starships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tble_name = \"starships\"\n",
    "starships = data_union[\"starships\"]\n",
    "starships.drop_duplicates(keep='first',inplace=True)\n",
    "starships.duplicated()\n",
    "starships.to_sql(tble_name,connection.get_connection(),schema=rfnd_schema,if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with Connector object\n",
    "connection.schema = rfnd_schema\n",
    "connection.create_table(tble_name,starships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"SELECT * FROM {rfnd_schema}.{tble_name}\"\n",
    "unique_starships = connection.execute_sql(sql_query)\n",
    "print(unique_starships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Actualiza los registros del tercer dataset (inciso/indicación 2)  \n",
    "actualizando el campo Starships con el valor Slave I para todos los valores del campo name iguales a Jango Fett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_nm = table_union\n",
    "values = {\"starships\":\"Slave I\"}\n",
    "where = {\"name\":\"Jango Fett\"}\n",
    "connection.update(table_nm=table_nm,schema=rfnd_schema,where=where,values=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Generar una consulta en dónde muestre el conteo de registros agrupados por las columnas Skin_color, eye_color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = (\n",
    "    \"SELECT skin_color, eye_color, count(*) \"\n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"GROUP BY skin_color, eye_color;\"\n",
    ")\n",
    "\n",
    "skin_eye_color = pd.read_sql_query(sql_query,connection.get_connection())\n",
    "print(skin_eye_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = (\n",
    "    \"SELECT skin_color, eye_color, count(*) \"\n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"GROUP BY skin_color, eye_color;\"\n",
    ")\n",
    "\n",
    "skin_eye_color = connection.execute_sql(sql_query)\n",
    "print(skin_eye_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Escribir una consulta SQL para calcular la altura promedio, la altura máxima y la altura mínima por cada especie (species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = (\n",
    "    \"SELECT species, AVG(height) avg_height, MAX(height) max_height, MIN(height) min_height \"\n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"GROUP BY species;\"\n",
    ")\n",
    "\n",
    "height_metrics = pd.read_sql_query(sql_query,connection.get_connection())\n",
    "print(height_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution wiht Connector object\n",
    "sql_query = (\n",
    "    \"SELECT species, AVG(height) avg_height, MAX(height) max_height, MIN(height) min_height \"\n",
    "    f\"FROM {rfnd_schema}.{table_union} \"\n",
    "    \"GROUP BY species;\"\n",
    ")\n",
    "\n",
    "height_metrics = connection.execute_sql(sql_query)\n",
    "print(height_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
